{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0c30c2",
   "metadata": {},
   "source": [
    "# Class 5: Scraping Web Data 1 - BeautifulSoup & HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc95920",
   "metadata": {},
   "source": [
    "## How do websites work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cabdd",
   "metadata": {},
   "source": [
    "### What is the \"backend\" of a website?\n",
    "The backend of a website has a bunch of moving parts. Content probably lives in a database (or, more likely these days, several databases). There are servers (big computers) with functions that are responsible for getting content from the databases and sending it, in a machine-readable format, to the frontend. This functionality is called an API, or *Application Programming Interface*. It is (generally speaking) how computers request and get data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d66073",
   "metadata": {},
   "source": [
    "### What is the \"frontend\" of a website?\n",
    "The frontend of a website is where you, the human user, see the content. The frontend takes the big chunk of content sent from the server via the API and puts it into a nice, pretty format. This is what you see as \"the website.\" The way that websites typically display content is via a combination of HTML (hypertext markup language) and Javascript, for the interactive features. \n",
    "#### HTML\n",
    "HTML is a programming language that people use to make webpages. It consists of *elements* that can be nested within each other. Elements are indicated with *tags;* typically there are open tags (`<p>`) and close tags (`</p>`) that surround the contents of an element. Browsers read HTML and use the tags to figure out how to display content; you don't see the raw HTML when you use a browser (though you can do so using the \"inspect element\" feature). Here is an example of a (very bare-bones) HTML file:\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>This is a header!</head>\n",
    "<body>\n",
    "\n",
    "<h1>This is a heading!</h1>\n",
    "\n",
    "<p>And this is a paragraph</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "Let's see what this looks like in our notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0ef851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>This is a header!</head>\n",
       "<body>\n",
       "\n",
       "<h1>This is a heading!</h1>\n",
       "\n",
       "<p>And this is a paragraph</p>\n",
       "\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "my_html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>This is a header!</head>\n",
    "<body>\n",
    "\n",
    "<h1>This is a heading!</h1>\n",
    "\n",
    "<p>And this is a paragraph</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(my_html_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e100b",
   "metadata": {},
   "source": [
    "Here's an example with a link and an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0265bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>This has a link!</head>\n",
       "<p>\n",
       "<a href=\"https://northeastern.edu\">This is a link</a>\n",
       "</p>\n",
       "<img src=\"images/whale.jpg\" alt=\"this is a whale\" width=200 height=200>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "now_with_link = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>This has a link!</head>\n",
    "<p>\n",
    "<a href=\"https://northeastern.edu\">This is a link</a>\n",
    "</p>\n",
    "<img src=\"images/whale.jpg\" alt=\"this is a whale\" width=200 height=200>\n",
    "\"\"\"\n",
    "display(HTML(now_with_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0b22b",
   "metadata": {},
   "source": [
    "Obviously most websites are more fancy than that, but at their core, when you visit them, HTML is being generated -- and you can look at it with your computer instead of via your browser. \n",
    "The act of looking at web pages via your computer (i.e. programatically) instead of via a conventional browser is called *scraping*, and it's not super hard to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2105",
   "metadata": {},
   "source": [
    "## Ways to access a website\n",
    "### Visiting the website via a browser \n",
    "Pros: \n",
    "* Does not require that much specialized knowledge.\n",
    "* Is how you're generally encouraged to use websites.\n",
    "* Easy to understand what you're looking at.\n",
    "\n",
    "Cons:\n",
    "* Does not scale well (if you're trying to look at 5000 webpages, this is not a good approach)\n",
    "\n",
    "### Using a website's API\n",
    "Pros:\n",
    "* Much faster\n",
    "* Scales better\n",
    "* Output is easily machine-readable\n",
    "\n",
    "Cons:\n",
    "* The API exists because the website's owner allows it to exist (see: Twitter/X). \n",
    "* Might cost money\n",
    "* Might have rate limits\n",
    "* Output is not easy to read if you are a human\n",
    "\n",
    "### Scraping a website\n",
    "Pros:\n",
    "* Also scales pretty well\n",
    "* Does not require the goodwill of a website's owner\n",
    "* Scraping publicly accessible data is [legal](https://techcrunch.com/2022/04/18/web-scraping-legal-court/) in the US\n",
    "\n",
    "Cons:\n",
    "* You run the risk of getting your IP banned\n",
    "* Often have to build a custom scraper for each website\n",
    "* Not doable for all websites (e.g. Facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b1b87",
   "metadata": {},
   "source": [
    "## How do we scrape a website?\n",
    "\n",
    "### First, we practice good robot citizenship via the `robots.txt` file!\n",
    "https://en.wikipedia.org/wiki/Robots_exclusion_standard\n",
    "\n",
    "http://www.robotstxt.org/robotstxt.html\n",
    "\n",
    "- It is a standard used by websites to communicate with web crawlers and other web robots\n",
    "- The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned\n",
    "- Robots are often used by search engines to categorize web sites\n",
    "- Not all robots cooperate with the standard; email harvesters, spambots, malware, and robots that scan for security vulnerabilities may even start with the portions of the website where they have been told to stay out\n",
    "\n",
    "In practice,\n",
    "- when a site owner wishes to give instructions to web robots they place a text file called robots.txt in the root of the web site hierarchy (e.g. https://www.example.com/robots.txt)\n",
    "- this text file contains the instructions in a specific format\n",
    "- robots that choose to follow the instructions try to fetch this file and read the instructions before fetching any other file from the web site\n",
    "- if this file doesn't exist, web robots assume that the web owner wishes to provide no specific instructions, and crawl the entire site.\n",
    "- a robots.txt file covers one origin. For websites with multiple subdomains, each subdomain must have its own robots.txt file.\n",
    "\n",
    "### Let's check out the `robots.txt` for tmz.com using the `requests` package.\n",
    "The `requests` package lets us make requests to websites or APIs. It gives us back HTML webpages that we can read through as if they were .html files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d11e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap: https://www.tmz.com/sitemaps/article/index.xml\n",
      "Sitemap: https://www.tmz.com/sitemaps/gallery/index.xml\n",
      "Sitemap: https://www.tmz.com/sitemaps/page/index.xml\n",
      "Sitemap: https://www.tmz.com/sitemaps/watch/index.xml\n",
      "Sitemap: https://www.tmz.com/sitemaps/news.xml\n",
      "\n",
      "User-agent: Googlebot-News\n",
      "Disallow: /photos\n",
      "Disallow: /videos\n",
      "\n",
      "User-agent: proximic\n",
      "Disallow:\n",
      "\n",
      "User-agent: bingbot\n",
      "Crawl-delay: 60\n",
      "\n",
      "User-agent: *\n",
      "\n",
      "Disallow: /_/\n",
      "Disallow: */print\n",
      "Disallow: /search\n",
      "Disallow: /xid\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('https://www.tmz.com/robots.txt')\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520161c3",
   "metadata": {},
   "source": [
    "Our `User-agent` is categorized under `*` because it is not Googlebot_news, proximic, or bingbot. This means we're not allowed to go to `tmz.com/*/print`, `tmz.com/search/`, or `tmz.com/xid`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f539cd3",
   "metadata": {},
   "source": [
    "## Actually Scraping Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fb133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00220069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
